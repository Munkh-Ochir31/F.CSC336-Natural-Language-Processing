{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA5E4U-BlOZv"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfcWpdUptioN"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bLAamdMtkmC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1765085177960,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "xUJENla2t2yY",
    "outputId": "20339106-5551-498b-957e-2d8a3742244a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"data\",\n  \"rows\": 49582,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49577,\n        \"samples\": [\n          \"sat enjoyable fast paced 45 mins roll br br roll country boy mat toby malone dreams becoming sports star mat travels city picked cousin george damien robertson well plan anyway george involved gangster tiny john batchelor making delivery needless say mat gets dragged george world br br thought great mat teaches george morals respect george teaches mat relax enjoy life little toby damien well cast together outstanding job br br every character movie complimented well two cops great david ngoombujarra brought great comic relief movie tiny played likable gangster reminded one favourite characters pando two hands br br one things liked roll showcased cities grew lived 20 years perth fremantle good see sights landmarks grew especially old ferris wheel br br rocks n rolls\",\n          \"twenty years ago five years old boy michael hawthorne witnessed father killing mother axe empty road committing suicide later present days michael gordon currie invites girlfriend peg stacy grant best friends chris myc agnew jennifer emmanuelle vaugier lisa ann kelly benson ned brendon beiser mitch maldive phillip rhys trish rachel hayward spend halloween country grandparents farm asks friends wear costumes would represent greatest innermost fear together indian friend crow byron chief moon would perform ancient indian celebration using carved wooden dummy morty jon fedele would eliminate fears forever greatest fear michael become serial killer like father something goes wrong morty turns father killing friends br br fear resurrection disappointing pointless slash movie uses interesting concept eliminating greatest innermost fear friend grows messy screenplay full clich exaggerated performances like example ms betsy palmer others weak general acting good unfortunately explanation dummy brought live spite surrounded close friends group feel pain sorrow one dies low pace along fifty minutes could used built better dramatic situation end michael shows charm father interested noticed along story know whether previous reference edited dvd released brazil 87 minutes running time special effects reasonable b movie vote four br br title brazil fear 2 uma noite de halloween fear 2 one night halloween\",\n          \"september 2003 36 year old jonny kennedy died terrible genetic condition called dystrophic epidermolysis bullosa eb meant skin literally fell slightest touch leaving body covered agonising sores leading final fight skin cancer last months jonny decided work filmmaker patrick collerton document life death result film first broadcast march uplifting confounding provocatively humorous story singular man shying away grim reality eb film also celebration life lived full produced directed patrick collerton first shown march 2004 boy whose skin fell become talked documentary year attracted nearly five million viewers screening public donated half million pounds jonny charity debra jonny kennedy memorial fund set raise another half million aim ensuring jonny kennedy left one million pound legacy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-bb808fc8-aa15-41c4-ac81-49c69fce5dad\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb808fc8-aa15-41c4-ac81-49c69fce5dad')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-bb808fc8-aa15-41c4-ac81-49c69fce5dad button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-bb808fc8-aa15-41c4-ac81-49c69fce5dad');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-f0253ea9-2be8-4ea8-93da-dc3bddbde5ed\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0253ea9-2be8-4ea8-93da-dc3bddbde5ed')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-f0253ea9-2be8-4ea8-93da-dc3bddbde5ed button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one reviewers mentioned watching 1 oz episode ...          1\n",
       "1  wonderful little production br br filming tech...          1\n",
       "2  thought wonderful way spend time hot summer we...          1\n",
       "3  basically family little boy jake thinks zombie...          0\n",
       "4  petter mattei love time money visually stunnin...          1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"C:\\\\Users\\\\my tech\\\\Documents\\\\1. 3A\\\\NLP\\\\data\\\\preprocessed_data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1765085179591,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "0n3G86sGQ1og",
    "outputId": "60bfcdbc-0a60-47b2-afe0-e30c5b2e5abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfnLORuF6SIt"
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-learn gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hHeqfOp55ti"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from gensim.models import Word2Vec\n",
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlFz6RFc6yPC"
   },
   "outputs": [],
   "source": [
    "# def create_doc_vec(doc_tokens, model, padding_size):\n",
    "#     vector_size = model.vector_size\n",
    "#     all_docs = []\n",
    "\n",
    "#     for tokens in doc_tokens:\n",
    "#         word_vectors = []\n",
    "\n",
    "#         for word in tokens:\n",
    "#             if word in model.wv:\n",
    "#                 word_vectors.append(model.wv[word])\n",
    "#             else:\n",
    "#                 word_vectors.append(np.zeros(vector_size))\n",
    "\n",
    "#         word_vectors = word_vectors[:padding_size]\n",
    "\n",
    "#         while len(word_vectors) < padding_size:\n",
    "#             word_vectors.append(np.zeros(vector_size))\n",
    "\n",
    "#         all_docs.append(np.array(word_vectors))\n",
    "\n",
    "#     return np.array(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iWa3Z-e8bbH"
   },
   "outputs": [],
   "source": [
    "X = data[\"review\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "x_tokens = [doc.split() for doc in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAs0l1MO9wyH"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzTs-IAI96N6"
   },
   "outputs": [],
   "source": [
    "def bert_tokenizer(tokenizer, data, max_length):\n",
    "  token = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "  for sent in data:\n",
    "    encoded_sent = tokenizer.encode_plus(\n",
    "        text=sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True\n",
    "      )\n",
    "\n",
    "    token[\"input_ids\"].append(torch.tensor(encoded_sent[\"input_ids\"]))\n",
    "    token[\"attention_mask\"].append(torch.tensor(encoded_sent[\"attention_mask\"]))\n",
    "\n",
    "  token[\"input_ids\"] = torch.stack(token[\"input_ids\"], dim=0)\n",
    "  token[\"attention_mask\"] = torch.stack(token[\"attention_mask\"], dim=0)\n",
    "  return token\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GA12IG0K8xXr"
   },
   "outputs": [],
   "source": [
    "def word_embed(device, tokenizer, bert, column, max_length=128, batch_size=8):\n",
    "  dataloader = DataLoader(dataset=column, batch_size=batch_size, shuffle=False)\n",
    "  otlst = []\n",
    "\n",
    "  for batch_data in tqdm.tqdm(dataloader, desc=\"bert\"):\n",
    "    batch_token = bert_tokenizer(tokenizer, batch_data, max_length)\n",
    "\n",
    "    for key in batch_token:\n",
    "      batch_token[key] = batch_token[key].to(device)\n",
    "\n",
    "    with torch.no_grad(): # model usage when using pytorch no training 1 token: 768 list\n",
    "      pool_out = bert(**batch_token)\n",
    "      \n",
    "      emb = pool_out.last_hidden_state \n",
    "    otlst.append(emb)\n",
    "\n",
    "  sumrslt = torch.cat(otlst, dim=0)\n",
    "  return sumrslt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355754,
     "status": "ok",
     "timestamp": 1765085552597,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "pQsdmb1HBbBX",
    "outputId": "ac30170f-d644-4abe-f9ad-813d7ef17233"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert: 100%|██████████| 6198/6198 [05:55<00:00, 17.43it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "result = word_embed(device, tokenizer, model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TL-QlRcWMWFw"
   },
   "outputs": [],
   "source": [
    "Y = torch.tensor(y)\n",
    "\n",
    "data = {\n",
    "    \"X\": result,\n",
    "    \"y\": Y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1765085572760,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "6cMxrXhZMvQ3",
    "outputId": "4925f9ba-af9f-48c1-e607-434acea8b5e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[-0.5991,  0.3871,  0.9998,  ...,  0.9999, -0.7852,  0.9860],\n",
       "         [-0.6411,  0.2918,  0.9997,  ...,  0.9999, -0.6456,  0.9784],\n",
       "         [-0.8663,  0.5999,  1.0000,  ...,  1.0000, -0.5957,  0.9981],\n",
       "         ...,\n",
       "         [-0.5383,  0.2810,  0.9995,  ...,  0.9998, -0.7829,  0.9757],\n",
       "         [-0.6088,  0.3436,  0.9997,  ...,  0.9999, -0.8002,  0.9918],\n",
       "         [-0.5629,  0.3054,  0.9995,  ...,  0.9998, -0.8059,  0.9747]],\n",
       "        device='cuda:0'),\n",
       " 'y': tensor([1, 1, 1,  ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1765085594444,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "lWZNSx_1THew",
    "outputId": "80b1e438-d81b-4386-aa84-e07d6b888901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"X\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1765085597424,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "l6944VNJTMqQ",
    "outputId": "c0bd5af4-5557-4736-f64d-ef5b05be6892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yl-DTekEfzZ"
   },
   "outputs": [],
   "source": [
    "torch.save(data, \"imdb_bert_embed.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5Z2P6G3Hujj"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.save(\"imdb_bert_embed.npy\", result.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUhxd9O4lQsb"
   },
   "source": [
    "# BERTS\n",
    "\n",
    "\n",
    "1. bert_base_cased\n",
    "2. bert-base-uncased\n",
    "3. Roberta\n",
    "4. SBert\n",
    "5. hateBert\n",
    "6. albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2761,
     "status": "ok",
     "timestamp": 1766028569645,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "e6Je7TjZoZ_H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\my tech\\AppData\\Local\\Temp\\ipykernel_9400\\332688062.py\", line 1, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModel\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_names = {\n",
    "    \"bert_base_cased\": \"bert-base-cased\",\n",
    "    \"bert_base_uncased\": \"google-bert/bert-base-uncased\"\n",
    "    #\"roberta\": \"FacebookAI/xlm-roberta-base\",\n",
    "    #\"sbert\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    #\"hatebert\": \"GroNLP/hateBERT\",\n",
    "    #\"albert\": \"albert-base-v2\"\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "models = {}\n",
    "\n",
    "for key, value in model_names.items():\n",
    "    tokenizers[key] = AutoTokenizer.from_pretrained(value)\n",
    "    models[key] = AutoModel.from_pretrained(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1766028582768,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "g-mLo0Kar1BN"
   },
   "outputs": [],
   "source": [
    "assert len(tokenizers.values()) == len(models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2776,
     "status": "ok",
     "timestamp": 1766028587097,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "a1Bk1fUVr5lw",
    "outputId": "7bf4a644-ffc0-46a0-9aea-d0b0fd6f0ca7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from google.colab import drive\n",
    "\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1157,
     "status": "ok",
     "timestamp": 1766028588262,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "hWUhTn3NscY3",
    "outputId": "71eda872-9a12-4bbd-a04b-1f166851cf4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one reviewers mentioned watching 1 oz episode ...          1\n",
       "1  wonderful little production br br filming tech...          1\n",
       "2  thought wonderful way spend time hot summer we...          1\n",
       "3  basically family little boy jake thinks zombie...          0\n",
       "4  petter mattei love time money visually stunnin...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\my tech\\\\Documents\\\\1. 3A\\\\NLP\\\\data\\\\preprocessed_data.csv\"\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1766028588290,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "eL2EI7HlsnWe",
    "outputId": "93b506c4-383a-4188-b22d-0d143dfe9429"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1766028589174,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "mO6c_jt3svXR"
   },
   "outputs": [],
   "source": [
    "X = data[\"review\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "x_tokens = [doc.split() for doc in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1766028589189,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "vd3DktI5_4U3"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.save(y, \"/content/drive/My Drive/Colab Notebooks/NLP/embedding/y.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766028589197,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "s5n8NPBOs5Q1"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def bert_tokenizer(tokenizer, data, max_length):\n",
    "    token = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": []\n",
    "        }\n",
    "\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        token[\"input_ids\"].append(torch.tensor(encoded_sent[\"input_ids\"]))\n",
    "        token[\"attention_mask\"].append(torch.tensor(encoded_sent[\"attention_mask\"]))\n",
    "\n",
    "    token[\"input_ids\"] = torch.stack(token[\"input_ids\"], dim=0)\n",
    "    token[\"attention_mask\"] = torch.stack(token[\"attention_mask\"], dim=0)\n",
    "    return token\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766028589215,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "yWIDnw_Ps7n8"
   },
   "outputs": [],
   "source": [
    "def word_embed(device, tokenizer, bert, column, max_length, batch_size=8):\n",
    "  dataloader = DataLoader(dataset=column, batch_size=batch_size, shuffle=False)\n",
    "  otlst = []\n",
    "\n",
    "  for batch_data in tqdm.tqdm(dataloader, desc=\"bert\"):\n",
    "    batch_token = bert_tokenizer(tokenizer, batch_data, max_length)\n",
    "\n",
    "    for key in batch_token:\n",
    "      batch_token[key] = batch_token[key].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**batch_token)\n",
    "\n",
    "        emb = outputs.last_hidden_state\n",
    "        emb = emb.half()  # float16 болгох (санах ой хэмнэх)\n",
    "        emb = emb.cpu()   # CPU руу буцаах\n",
    "\n",
    "    otlst.append(emb)\n",
    "\n",
    "  sumrslt = torch.cat(otlst, dim=0)\n",
    "  return sumrslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n",
      "cuda available: True\n",
      "cuda version: 12.1\n",
      "gpu count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda version:\", torch.version.cuda)\n",
    "print(\"gpu count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Upgrade for RTX 5060\n",
    "\n",
    "RTX 5060-д ажиллуулахын тулд PyTorch-ийг шинэчлэх хэрэгтэй.\n",
    "\n",
    "**Terminal дээр энэ командыг ажиллуулна уу:**\n",
    "```bash\n",
    "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch-ийг шинэчлэх (энэ нь удаан байж болно)\n",
    "# Terminal дээр ажиллуулна уу: pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# Шинэчилсний дараа Kernel-ийг restart хийнэ үү!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1766028590838,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "pss5BPcMtny_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# PyTorch шинэчлэсний дараа GPU ашиглана\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Y = torch.tensor(y.values, dtype=torch.long)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1766028591365,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "aT0uX4B23GP3",
    "outputId": "6793375c-96f9-45c5-9cc2-5f54c9e5251e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_save_path = \"C:\\\\Users\\\\my tech\\\\Documents\\\\1. 3A\\\\NLP\\\\data\\\\embedding\\\\embedded_x\"\n",
    "\n",
    "if os.path.exists(data_save_path):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1766028630457,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "kt06DkljgcbA",
    "outputId": "2e28d92d-602b-46c5-89f4-9b4b616c6a27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1782652,
     "status": "ok",
     "timestamp": 1766030426965,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "-4yn162ctqpf",
    "outputId": "5395a796-d62c-44cf-88c3-d07d71645085"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started embedding: bert_base_cased\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert:   0%|          | 0/6198 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started embedding: bert_base_cased\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert:   0%|          | 0/6198 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mword_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# embedded_data = {\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     \"X\": result,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     \"y\": y\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mword_embed\u001b[1;34m(device, tokenizer, bert, column, max_length, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m   batch_token[key] \u001b[38;5;241m=\u001b[39m batch_token[key]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m bert(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_token)\n\u001b[0;32m     14\u001b[0m     emb \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     15\u001b[0m     emb \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:936\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 936\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:179\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    176\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    182\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\my tech\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "for key, tokenizer in tokenizers.items():\n",
    "    model = models[key]\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Started embedding: {key}\\n\")\n",
    "    result = word_embed(device, tokenizer, model, X, 64)\n",
    "\n",
    "    print(f\"X: {result.shape}\")\n",
    "    print(f\"X: {result.dtype}\")\n",
    "    print(result.__len__())\n",
    "\n",
    "    torch.save(result ,f\"{data_save_path}/imdb_{key}_embed.pt\")\n",
    "    print(\"saved\\n\")\n",
    "\n",
    "    del model\n",
    "    del result\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "error",
     "timestamp": 1766030435902,
     "user": {
      "displayName": "Gantomor Nyam",
      "userId": "13382463166259925194"
     },
     "user_tz": -480
    },
    "id": "Qsg0DMgiI3Sj",
    "outputId": "3616509b-0278-4499-8300-544b6135a822"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (ipython-input-2131601395.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2131601395.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_bert_base_uncased_embed.pt,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_bert_base_cased_embed.pt\",\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_bert_base_uncased_embed.pt,\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_roberta_embed.pt\",\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_sbert_embed.pt\",\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_hatebert_embed.pt\",\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/NLP/embedding/embedded_x/imdb_albert_embed.pt\"\n",
    "]\n",
    "\n",
    "for data in data_paths:\n",
    "    pt = torch.load(data)\n",
    "    print(f\"shape: {pt.shape}\")\n",
    "    print(f\"dtype: {pt.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7M0J4_hZDf-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMcJOZJcfB7Ay71Nc0S1h9p",
   "collapsed_sections": [
    "RA5E4U-BlOZv"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
