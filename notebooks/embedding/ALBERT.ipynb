{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3576ad4d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba85564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2492d28",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d92df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../../data/cleaned_label.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5a555",
   "metadata": {},
   "source": [
    "## 3. Initialize ALBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name - ALBERT Base v2\n",
    "MODEL_NAME = 'albert-base-v2'\n",
    "\n",
    "print(f\"Loading ALBERT model: {MODEL_NAME}\")\n",
    "print(\"Note: ALBERT (A Lite BERT) uses parameter sharing\")\n",
    "print(\"      - Cross-layer parameter sharing\")\n",
    "print(\"      - Factorized embedding parameterization\")\n",
    "print(\"      - Only 12M parameters (10x smaller than BERT!)\")\n",
    "print(\"      - SOP (Sentence Order Prediction) instead of NSP\\n\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AlbertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AlbertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"Total parameters: ~12M (vs BERT's 110M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebb2c3",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_embedding(text, tokenizer, model, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract ALBERT [CLS] token embedding\n",
    "    \n",
    "    Args:\n",
    "        text: str - Input text\n",
    "        tokenizer: ALBERT tokenizer\n",
    "        model: ALBERT model\n",
    "        device: torch device\n",
    "        max_length: int - Max sequence length\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: 768-dimensional embedding vector\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract [CLS] token embedding (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Embedding extraction function defined!\")\n",
    "\n",
    "# Test on sample text\n",
    "sample_text = \"This movie is absolutely fantastic!\"\n",
    "sample_embedding = get_albert_embedding(sample_text, tokenizer, model, device)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Sample embedding preview (first 10 values): {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b4e9d",
   "metadata": {},
   "source": [
    "## 5. Extract Embeddings for All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32  # Process in batches to save memory\n",
    "\n",
    "print(\"Extracting ALBERT embeddings for all documents...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total documents: {len(df)}\\n\")\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    batch_texts = df['review_text'].iloc[i:i+BATCH_SIZE].values\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        embedding = get_albert_embedding(str(text), tokenizer, model, device, MAX_LENGTH)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"\\nEmbeddings extracted!\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Each document: {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77606062",
   "metadata": {},
   "source": [
    "## 6. Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = '../../models/albert_embeddings'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = os.path.join(output_dir, 'albert_embeddings.npy')\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# Save labels\n",
    "labels_path = os.path.join(output_dir, 'labels.npy')\n",
    "np.save(labels_path, df['sentiment_label'].values)\n",
    "print(f\"Labels saved: {labels_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_type': 'albert-base-v2',\n",
    "    'hidden_size': model.config.hidden_size,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'num_documents': len(embeddings),\n",
    "    'embedding_shape': embeddings.shape,\n",
    "    'extraction_method': '[CLS] token from last_hidden_state',\n",
    "    'parameters': '~12M (parameter sharing)',\n",
    "    'notes': 'ALBERT: Cross-layer parameter sharing, factorized embeddings, SOP task'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, 'metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - albert_embeddings.npy    (Embeddings: {embeddings.shape})\")\n",
    "print(f\"  - labels.npy               (Sentiment labels)\")\n",
    "print(f\"  - metadata.pkl             (Model metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c59486",
   "metadata": {},
   "source": [
    "## 7. Verify Saved Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings\n",
    "print(\"Loading saved embeddings...\\n\")\n",
    "\n",
    "loaded_embeddings = np.load(embeddings_path)\n",
    "loaded_labels = np.load(labels_path)\n",
    "\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    loaded_metadata = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Embeddings loaded: {loaded_embeddings.shape}\")\n",
    "print(f\"‚úì Labels loaded: {loaded_labels.shape}\")\n",
    "print(f\"\\n‚úì Metadata:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Verify integrity\n",
    "print(f\"\\n‚úì Verification:\")\n",
    "print(f\"    Embeddings match: {np.allclose(embeddings, loaded_embeddings)}\")\n",
    "print(f\"    Labels match: {np.array_equal(df['sentiment_label'].values, loaded_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4e26c",
   "metadata": {},
   "source": [
    "## 8. Embedding Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaecbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Embedding Statistics:\\n\")\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"Std: {embeddings.std():.4f}\")\n",
    "print(f\"Min: {embeddings.min():.4f}\")\n",
    "print(f\"Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(embeddings.flatten(), bins=100, alpha=0.7, color='red')\n",
    "axes[0].set_xlabel('Embedding Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('ALBERT Embedding Value Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean embedding per document\n",
    "mean_embeddings = embeddings.mean(axis=1)\n",
    "axes[1].hist(mean_embeddings, bins=50, alpha=0.7, color='darkred')\n",
    "axes[1].set_xlabel('Mean Embedding Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Mean Embedding per Document')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistics plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98a534",
   "metadata": {},
   "source": [
    "## 9. Compare Positive vs Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c26292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample for visualization\n",
    "sample_size = 1000\n",
    "indices = np.random.choice(len(embeddings), size=min(sample_size, len(embeddings)), replace=False)\n",
    "\n",
    "sample_embeddings = embeddings[indices]\n",
    "sample_labels = df['sentiment_label'].iloc[indices].values\n",
    "\n",
    "# PCA reduction to 2D\n",
    "print(f\"Performing PCA on {len(sample_embeddings)} samples...\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(sample_embeddings)\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=sample_labels,\n",
    "    cmap='RdYlGn',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "plt.colorbar(scatter, label='Sentiment (0=Neg, 1=Pos)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('ALBERT Embeddings Visualization (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2619d4",
   "metadata": {},
   "source": [
    "## 10. ALBERT vs BERT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALBERT vs BERT Differences\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = {\n",
    "    'Feature': [\n",
    "        'Full Name',\n",
    "        'Parameters (Base)',\n",
    "        'Embedding Size',\n",
    "        'Hidden Size',\n",
    "        'Parameter Sharing',\n",
    "        'Embedding Factorization',\n",
    "        'Training Task',\n",
    "        'Memory Usage',\n",
    "        'Training Speed',\n",
    "        'Performance'\n",
    "    ],\n",
    "    'BERT': [\n",
    "        'Bidirectional Encoder Representations from Transformers',\n",
    "        '110M',\n",
    "        '768',\n",
    "        '768',\n",
    "        'No sharing',\n",
    "        'No factorization',\n",
    "        'MLM + NSP',\n",
    "        'High',\n",
    "        'Slower',\n",
    "        'Good'\n",
    "    ],\n",
    "    'ALBERT': [\n",
    "        'A Lite BERT',\n",
    "        '12M (10x smaller!)',\n",
    "        '128 (factorized)',\n",
    "        '768',\n",
    "        'Cross-layer parameter sharing',\n",
    "        'Yes (E=128, H=768)',\n",
    "        'MLM + SOP',\n",
    "        'Low',\n",
    "        'Faster',\n",
    "        'Better (with fewer params!)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Innovations of ALBERT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Factorized Embedding Parameterization:\")\n",
    "print(\"   - Separates embedding size (E) from hidden size (H)\")\n",
    "print(\"   - E=128, H=768 instead of E=H=768\")\n",
    "print(\"   - Reduces parameters: 30000*768 ‚Üí 30000*128 + 128*768\")\n",
    "\n",
    "print(\"\\n2. Cross-Layer Parameter Sharing:\")\n",
    "print(\"   - All layers share the same parameters\")\n",
    "print(\"   - Only 1 transformer layer is stored\")\n",
    "print(\"   - Massive parameter reduction\")\n",
    "\n",
    "print(\"\\n3. Sentence Order Prediction (SOP):\")\n",
    "print(\"   - Replaces NSP (Next Sentence Prediction)\")\n",
    "print(\"   - More challenging: predict if sentence order is correct\")\n",
    "print(\"   - Better inter-sentence coherence understanding\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Result: 10x fewer parameters, better performance!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b7c1c",
   "metadata": {},
   "source": [
    "## 11. Parameter Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edeb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter counts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Parameter Efficiency Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vocab_size = 30000\n",
    "hidden_size = 768\n",
    "embedding_size_albert = 128\n",
    "num_layers = 12\n",
    "\n",
    "# BERT embedding parameters\n",
    "bert_embedding_params = vocab_size * hidden_size\n",
    "print(f\"\\nBERT Embedding Parameters:\")\n",
    "print(f\"  {vocab_size:,} √ó {hidden_size} = {bert_embedding_params:,}\")\n",
    "\n",
    "# ALBERT embedding parameters (factorized)\n",
    "albert_vocab_to_embed = vocab_size * embedding_size_albert\n",
    "albert_embed_to_hidden = embedding_size_albert * hidden_size\n",
    "albert_embedding_params = albert_vocab_to_embed + albert_embed_to_hidden\n",
    "print(f\"\\nALBERT Embedding Parameters (Factorized):\")\n",
    "print(f\"  Vocab‚ÜíEmbed: {vocab_size:,} √ó {embedding_size_albert} = {albert_vocab_to_embed:,}\")\n",
    "print(f\"  Embed‚ÜíHidden: {embedding_size_albert} √ó {hidden_size} = {albert_embed_to_hidden:,}\")\n",
    "print(f\"  Total: {albert_embedding_params:,}\")\n",
    "print(f\"  Reduction: {bert_embedding_params / albert_embedding_params:.1f}x\")\n",
    "\n",
    "# Layer parameters (simplified)\n",
    "approx_layer_params = 7_000_000  # Approximate per layer\n",
    "\n",
    "bert_total = bert_embedding_params + (num_layers * approx_layer_params)\n",
    "albert_total = albert_embedding_params + approx_layer_params  # Shared layers\n",
    "\n",
    "print(f\"\\nTotal Approximate Parameters:\")\n",
    "print(f\"  BERT: {bert_total:,} (~110M)\")\n",
    "print(f\"  ALBERT: {albert_total:,} (~12M)\")\n",
    "print(f\"  Overall Reduction: {bert_total / albert_total:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALBERT achieves ~10x parameter reduction while maintaining performance!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383698f",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALBERT EMBEDDING EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Model: {MODEL_NAME}\")\n",
    "print(f\"üöÄ Architecture: A Lite BERT (Parameter-efficient)\")\n",
    "print(f\"üìè Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"üìÅ Total documents: {embeddings.shape[0]:,}\")\n",
    "print(f\"üíæ Total size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"‚ö° Parameters: Only 12M (10x smaller than BERT!)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Files saved:\")\n",
    "print(f\"   {output_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ albert_embeddings.npy  ({embeddings.shape})\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ labels.npy             ({loaded_labels.shape})\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ metadata.pkl\")\n",
    "\n",
    "print(f\"\\nüí° Usage:\")\n",
    "print(f\"   These embeddings can be used with any classifier:\")\n",
    "print(f\"   - Logistic Regression\")\n",
    "print(f\"   - SVM\")\n",
    "print(f\"   - Random Forest\")\n",
    "print(f\"   - Neural Networks\")\n",
    "\n",
    "print(f\"\\nüîß Load embeddings:\")\n",
    "print(f\"   X = np.load('{embeddings_path}')\")\n",
    "print(f\"   y = np.load('{labels_path}')\")\n",
    "\n",
    "print(f\"\\n‚ö° Why ALBERT:\")\n",
    "print(f\"   - 10x fewer parameters than BERT\")\n",
    "print(f\"   - Parameter sharing across layers\")\n",
    "print(f\"   - Factorized embeddings\")\n",
    "print(f\"   - Better SOP task vs NSP\")\n",
    "print(f\"   - Faster training, less memory\")\n",
    "print(f\"   - Often matches or exceeds BERT performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALBERT embedding extraction complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
