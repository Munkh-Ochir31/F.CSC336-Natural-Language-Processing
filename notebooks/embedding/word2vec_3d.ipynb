{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4902ca98",
   "metadata": {},
   "source": [
    "# Word2Vec 3D Embeddings for LSTM\n",
    "\n",
    "Энэ notebook нь текстийн өгөгдлийг word-level дээр боловсруулж, 3D tensor (samples, max_length, embedding_dim) үүсгэх замаар LSTM-д тохирсон форматтай болгоно.\n",
    "\n",
    "**Алхамууд:**\n",
    "1. Текст цэвэрлэх\n",
    "2. Word tokenization\n",
    "3. Word2Vec модел сургах\n",
    "4. 3D embedding tensor үүсгэх\n",
    "5. Хадгалах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3357cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Сангууд ачааллаа\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b309528",
   "metadata": {},
   "source": [
    "## 1. Өгөгдөл ачаалах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Өгөгдөл ачаалах\n",
    "df = pd.read_csv('/home/tr1bo/Documents/4. 3A/Эх хэлний боловсруулалт/biydaalt1/data/cleaned_label.csv')\n",
    "\n",
    "print(f\"Нийт мөрийн тоо: {len(df)}\")\n",
    "print(f\"Багана: {df.columns.tolist()}\")\n",
    "print(\"\\nЭхний хэдэн мөр:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7396451",
   "metadata": {},
   "source": [
    "## 2. Текст цэвэрлэх ба Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b229b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Текст цэвэрлэх функц\"\"\"\n",
    "    # Жижиг үсэг болгох\n",
    "    text = text.lower()\n",
    "    # HTML тэмдэгт устгах\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Зөвхөн үсэг, тоо, зай үлдээх\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Олон зай нэг зай болгох\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Текст цэвэрлэх\n",
    "print(\"Текст цэвэрлэж байна...\")\n",
    "df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "# Хоосон мөр устгах\n",
    "df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Цэвэрлэсний дараах мөрийн тоо: {len(df)}\")\n",
    "print(\"\\nЖишээ:\")\n",
    "print(f\"Анхны: {df['review_text'].iloc[0][:100]}...\")\n",
    "print(f\"Цэвэр: {df['cleaned_text'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6254a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Үгэнд хувааж list болгох\n",
    "print(\"Tokenization хийж байна...\")\n",
    "tokenized_texts = [text.split() for text in tqdm(df['cleaned_text'])]\n",
    "\n",
    "# Статистик\n",
    "lengths = [len(tokens) for tokens in tokenized_texts]\n",
    "print(f\"\\nТекст бүрийн дундаж үг: {np.mean(lengths):.2f}\")\n",
    "print(f\"Хамгийн богино: {np.min(lengths)} үг\")\n",
    "print(f\"Хамгийн урт: {np.max(lengths)} үг\")\n",
    "print(f\"Median: {np.median(lengths):.2f} үг\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d76e74",
   "metadata": {},
   "source": [
    "## 3. Word2Vec модел сургах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec параметрүүд\n",
    "EMBEDDING_DIM = 100  # Embedding хэмжээ\n",
    "WINDOW_SIZE = 5      # Context цонхны хэмжээ\n",
    "MIN_COUNT = 2        # Хамгийн багадаа 2 удаа давтагдах үг\n",
    "\n",
    "print(\"Word2Vec модел сургаж байна...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=4,\n",
    "    sg=0,  # CBOW\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"\\nYocabulary хэмжээ: {len(w2v_model.wv)}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "\n",
    "# Модел хадгалах\n",
    "model_dir = '/home/tr1bo/Documents/4. 3A/Эх хэлний боловсруулалт/biydaalt1/models/word2vec'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'word2vec_3d.model')\n",
    "w2v_model.save(model_path)\n",
    "print(f\"\\nWord2Vec модел хадгалагдсан: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f9acd",
   "metadata": {},
   "source": [
    "## 4. 3D Embedding Tensor үүсгэх\n",
    "\n",
    "LSTM-д оролт өгөхийн тулд өгөгдлийг **(samples, max_length, embedding_dim)** хэлбэрт оруулах шаардлагатай."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length тодорхойлох\n",
    "MAX_LENGTH = 200  # Ихэнх текстийг хамрах утга\n",
    "\n",
    "print(f\"Maximum sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Энэ нь текстийн {np.percentile(lengths, 95):.0f} percentile-ээс их байна\")\n",
    "\n",
    "# OOV (Out of Vocabulary) үгэнд зориулсан нөөц вектор\n",
    "oov_vector = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "def text_to_embedding_sequence(tokens, max_length):\n",
    "    \"\"\"\n",
    "    Tokenized текстийг embedding sequence болгох\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        2D numpy array (max_length, embedding_dim)\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    \n",
    "    for word in tokens[:max_length]:  # Maximum length хүртэл авах\n",
    "        if word in w2v_model.wv:\n",
    "            sequence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            sequence.append(oov_vector)  # OOV үгэд нөөц вектор\n",
    "    \n",
    "    # Padding: богино текстийг нөхөх\n",
    "    while len(sequence) < max_length:\n",
    "        sequence.append(oov_vector)\n",
    "    \n",
    "    return np.array(sequence)\n",
    "\n",
    "# 3D tensor үүсгэх\n",
    "print(\"\\n3D embedding tensor үүсгэж байна...\")\n",
    "embeddings_3d = []\n",
    "\n",
    "for tokens in tqdm(tokenized_texts):\n",
    "    emb_seq = text_to_embedding_sequence(tokens, MAX_LENGTH)\n",
    "    embeddings_3d.append(emb_seq)\n",
    "\n",
    "embeddings_3d = np.array(embeddings_3d)\n",
    "\n",
    "print(f\"\\n3D Embedding хэмжээ: {embeddings_3d.shape}\")\n",
    "print(f\"  - Samples: {embeddings_3d.shape[0]}\")\n",
    "print(f\"  - Max Length (timesteps): {embeddings_3d.shape[1]}\")\n",
    "print(f\"  - Embedding Dimension: {embeddings_3d.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813c166",
   "metadata": {},
   "source": [
    "## 5. Labels бэлтгэх"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72397d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels ачаалах\n",
    "labels = df['sentiment_label'].values\n",
    "\n",
    "print(f\"Labels хэмжээ: {labels.shape}\")\n",
    "print(f\"Positive: {np.sum(labels == 1)}\")\n",
    "print(f\"Negative: {np.sum(labels == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909c1ca",
   "metadata": {},
   "source": [
    "## 6. Хадгалах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D embeddings хадгалах\n",
    "embeddings_path = os.path.join(model_dir, 'word2vec_3d_embeddings.npy')\n",
    "np.save(embeddings_path, embeddings_3d)\n",
    "print(f\"3D Embeddings хадгалагдсан: {embeddings_path}\")\n",
    "\n",
    "# Labels хадгалах\n",
    "labels_path = os.path.join(model_dir, 'word2vec_3d_labels.npy')\n",
    "np.save(labels_path, labels)\n",
    "print(f\"Labels хадгалагдсан: {labels_path}\")\n",
    "\n",
    "# Metadata хадгалах\n",
    "metadata = {\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'vocab_size': len(w2v_model.wv),\n",
    "    'num_samples': len(embeddings_3d),\n",
    "    'shape': embeddings_3d.shape\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, 'word2vec_3d_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata хадгалагдсан: {metadata_path}\")\n",
    "\n",
    "print(\"\\n✅ Бүх файлууд амжилттай хадгалагдлаа!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32519f",
   "metadata": {},
   "source": [
    "## 7. Хадгалсан файлуудыг шалгах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файлуудыг дахин ачаалж шалгах\n",
    "print(\"Файлуудыг дахин ачаалж байна...\")\n",
    "\n",
    "# Embeddings ачаалах\n",
    "X_loaded = np.load(embeddings_path)\n",
    "print(f\"✓ 3D Embeddings: {X_loaded.shape}\")\n",
    "\n",
    "# Labels ачаалах\n",
    "y_loaded = np.load(labels_path)\n",
    "print(f\"✓ Labels: {y_loaded.shape}\")\n",
    "\n",
    "# Metadata ачаалах\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    meta_loaded = pickle.load(f)\n",
    "print(f\"✓ Metadata:\")\n",
    "for key, value in meta_loaded.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(\"\\n✅ Бүх файлууд зөв ачааллагдлаа!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56543092",
   "metadata": {},
   "source": [
    "## 8. LSTM-д ашиглах жишээ\n",
    "\n",
    "Үүсгэсэн 3D embedding-ийг LSTM модел дээр ашиглах жишээ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test хуваах\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_loaded, y_loaded, test_size=0.2, random_state=42, stratify=y_loaded\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# LSTM модел үүсгэх\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=False), input_shape=(MAX_LENGTH, EMBEDDING_DIM)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nМоделийн архитектур:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n✅ LSTM модел бэлэн! model.fit() дуудаж сургаж болно.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
