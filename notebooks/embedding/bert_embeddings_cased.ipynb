{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a61ac79",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2aa52b",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../../data/cleaned_label.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace5abf",
   "metadata": {},
   "source": [
    "## 3. Initialize BERT Base Cased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd37fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name - BERT Base Cased (preserves capitalization)\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "print(f\"Loading BERT Cased model: {MODEL_NAME}\")\n",
    "print(\"Note: This model preserves case information (uppercase/lowercase)\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223300b",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f03345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text, tokenizer, model, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract BERT Cased [CLS] token embedding for a text\n",
    "    \n",
    "    Args:\n",
    "        text: str - Input text (case preserved)\n",
    "        tokenizer: BERT tokenizer\n",
    "        model: BERT model\n",
    "        device: torch device\n",
    "        max_length: int - Max sequence length\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: 768-dimensional embedding vector\n",
    "    \"\"\"\n",
    "    # Tokenize (case preserved)\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract [CLS] token embedding (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Embedding extraction function defined!\")\n",
    "\n",
    "# Test on sample text (note case sensitivity)\n",
    "sample_text = \"This MOVIE is GREAT!\"\n",
    "sample_embedding = get_bert_embedding(sample_text, tokenizer, model, device)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Sample embedding preview (first 10 values): {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d002e24c",
   "metadata": {},
   "source": [
    "## 5. Extract Embeddings for All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32  # Process in batches to save memory\n",
    "\n",
    "print(\"Extracting BERT Cased embeddings for all documents...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total documents: {len(df)}\\n\")\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    batch_texts = df['review_text'].iloc[i:i+BATCH_SIZE].values\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        embedding = get_bert_embedding(str(text), tokenizer, model, device, MAX_LENGTH)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"\\nEmbeddings extracted!\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Each document: {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293d1b5",
   "metadata": {},
   "source": [
    "## 6. Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = '../../models/bert_cased_embeddings'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = os.path.join(output_dir, 'bert_cased_embeddings.npy')\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# Save labels\n",
    "labels_path = os.path.join(output_dir, 'labels.npy')\n",
    "np.save(labels_path, df['sentiment_label'].values)\n",
    "print(f\"Labels saved: {labels_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_type': 'bert-base-cased',\n",
    "    'case_sensitive': True,\n",
    "    'hidden_size': model.config.hidden_size,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'num_documents': len(embeddings),\n",
    "    'embedding_shape': embeddings.shape,\n",
    "    'extraction_method': '[CLS] token from last_hidden_state'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, 'metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - bert_cased_embeddings.npy    (Embeddings: {embeddings.shape})\")\n",
    "print(f\"  - labels.npy                   (Sentiment labels)\")\n",
    "print(f\"  - metadata.pkl                 (Model metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37d82e",
   "metadata": {},
   "source": [
    "## 7. Verify Saved Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings\n",
    "print(\"Loading saved embeddings...\\n\")\n",
    "\n",
    "loaded_embeddings = np.load(embeddings_path)\n",
    "loaded_labels = np.load(labels_path)\n",
    "\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    loaded_metadata = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Embeddings loaded: {loaded_embeddings.shape}\")\n",
    "print(f\"‚úì Labels loaded: {loaded_labels.shape}\")\n",
    "print(f\"\\n‚úì Metadata:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Verify integrity\n",
    "print(f\"\\n‚úì Verification:\")\n",
    "print(f\"    Embeddings match: {np.allclose(embeddings, loaded_embeddings)}\")\n",
    "print(f\"    Labels match: {np.array_equal(df['sentiment_label'].values, loaded_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdfb90",
   "metadata": {},
   "source": [
    "## 8. Embedding Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Embedding Statistics:\\n\")\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"Std: {embeddings.std():.4f}\")\n",
    "print(f\"Min: {embeddings.min():.4f}\")\n",
    "print(f\"Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(embeddings.flatten(), bins=100, alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Embedding Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('BERT Cased Embedding Value Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean embedding per document\n",
    "mean_embeddings = embeddings.mean(axis=1)\n",
    "axes[1].hist(mean_embeddings, bins=50, alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Mean Embedding Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Mean Embedding per Document')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistics plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8a5e6",
   "metadata": {},
   "source": [
    "## 9. Compare Positive vs Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample for visualization\n",
    "sample_size = 1000\n",
    "indices = np.random.choice(len(embeddings), size=min(sample_size, len(embeddings)), replace=False)\n",
    "\n",
    "sample_embeddings = embeddings[indices]\n",
    "sample_labels = df['sentiment_label'].iloc[indices].values\n",
    "\n",
    "# PCA reduction to 2D\n",
    "print(f\"Performing PCA on {len(sample_embeddings)} samples...\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(sample_embeddings)\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=sample_labels,\n",
    "    cmap='RdYlGn',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "plt.colorbar(scatter, label='Sentiment (0=Neg, 1=Pos)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('BERT Cased Embeddings Visualization (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f355cd",
   "metadata": {},
   "source": [
    "## 10. Case Sensitivity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1edee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case sensitivity\n",
    "print(\"Testing case sensitivity of BERT Cased model:\\n\")\n",
    "\n",
    "test_sentences = [\n",
    "    (\"this movie is great\", \"This movie is great\"),\n",
    "    (\"the movie was EXCELLENT\", \"the movie was excellent\"),\n",
    "    (\"I LOVED IT\", \"i loved it\")\n",
    "]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for lower, upper in test_sentences:\n",
    "    emb1 = get_bert_embedding(lower, tokenizer, model, device)\n",
    "    emb2 = get_bert_embedding(upper, tokenizer, model, device)\n",
    "    \n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    print(f\"Sentence 1: '{lower}'\")\n",
    "    print(f\"Sentence 2: '{upper}'\")\n",
    "    print(f\"Cosine similarity: {similarity:.4f}\")\n",
    "    print(f\"Different vectors: {not np.allclose(emb1, emb2)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nNote: BERT Cased treats uppercase/lowercase differently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166d438",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BERT BASE CASED EMBEDDING EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Model: {MODEL_NAME}\")\n",
    "print(f\"üî§ Case Sensitive: YES (preserves capitalization)\")\n",
    "print(f\"üìè Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"üìÅ Total documents: {embeddings.shape[0]:,}\")\n",
    "print(f\"üíæ Total size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Files saved:\")\n",
    "print(f\"   {output_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ bert_cased_embeddings.npy  ({embeddings.shape})\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ labels.npy                 ({loaded_labels.shape})\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ metadata.pkl\")\n",
    "\n",
    "print(f\"\\nüí° Usage:\")\n",
    "print(f\"   These embeddings can be used with any classifier:\")\n",
    "print(f\"   - Logistic Regression\")\n",
    "print(f\"   - SVM\")\n",
    "print(f\"   - Random Forest\")\n",
    "print(f\"   - Neural Networks\")\n",
    "\n",
    "print(f\"\\nüîß Load embeddings:\")\n",
    "print(f\"   X = np.load('{embeddings_path}')\")\n",
    "print(f\"   y = np.load('{labels_path}')\")\n",
    "\n",
    "print(f\"\\n‚ö° Difference from bert-base-uncased:\")\n",
    "print(f\"   - Preserves uppercase/lowercase information\")\n",
    "print(f\"   - Better for texts where case matters (e.g., proper nouns)\")\n",
    "print(f\"   - May perform better on sentiment (e.g., 'GREAT' vs 'great')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BERT Cased embedding extraction complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
