{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d355f664",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a44b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911040a",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f1f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 2)\n",
      "\n",
      "Columns: ['review_text', 'sentiment_label']\n",
      "\n",
      "First few rows:\n",
      "                                         review_text  sentiment_label\n",
      "0  Once again Mr. Costner has dragged out a movie...                0\n",
      "1  This is a pale imitation of 'Officer and a Gen...                0\n",
      "2  Years ago, when DARLING LILI played on TV, it ...                0\n",
      "3  I was looking forward to this movie. Trustwort...                0\n",
      "4  First of all, I would like to say that I am a ...                0\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "0    25000\n",
      "1    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv('../../data/cleaned_label.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca659c6",
   "metadata": {},
   "source": [
    "## 3. Tokenize Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86547e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 40265.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total documents: 50000\n",
      "Sample tokenized text: ['once', 'again', 'mr.', 'costner', 'has', 'dragged', 'out', 'a', 'movie', 'for', 'far', 'longer', 'than', 'necessary.', 'aside', 'from', 'the', 'terrific', 'sea', 'rescue']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenization (split by space)\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Split text into words\"\"\"\n",
    "    return str(text).lower().split()\n",
    "\n",
    "# Tokenize all texts\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenized_texts = [simple_tokenize(text) for text in tqdm(df['review_text'])]\n",
    "\n",
    "print(f\"\\nTotal documents: {len(tokenized_texts)}\")\n",
    "print(f\"Sample tokenized text: {tokenized_texts[0][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdc264",
   "metadata": {},
   "source": [
    "## 4. Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568e7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Parameters:\n",
      "  - Vector size: 100\n",
      "  - Window: 5\n",
      "  - Min count: 2\n",
      "  - Epochs: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec model trained successfully!\n",
      "Vocabulary size: 149452\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec parameters\n",
    "VECTOR_SIZE = 100      # Embedding dimension\n",
    "WINDOW = 5             # Context window size\n",
    "MIN_COUNT = 2          # Minimum word frequency\n",
    "WORKERS = 4            # Number of threads\n",
    "EPOCHS = 10            # Training epochs\n",
    "\n",
    "print(\"Training Word2Vec model...\")\n",
    "print(f\"Parameters:\")\n",
    "print(f\"  - Vector size: {VECTOR_SIZE}\")\n",
    "print(f\"  - Window: {WINDOW}\")\n",
    "print(f\"  - Min count: {MIN_COUNT}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print()\n",
    "\n",
    "# Train model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS,\n",
    "    epochs=EPOCHS,\n",
    "    sg=0  # 0=CBOW, 1=Skip-gram\n",
    ")\n",
    "\n",
    "print(\"\\nWord2Vec model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508e1da",
   "metadata": {},
   "source": [
    "## 5. Generate Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f025bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:15<00:00, 3151.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings shape: (50000, 100)\n",
      "Each document represented as 100-dimensional vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_document_embedding(tokens, model):\n",
    "    \"\"\"\n",
    "    Get document embedding by averaging word vectors\n",
    "    \n",
    "    Args:\n",
    "        tokens: list of words\n",
    "        model: trained Word2Vec model\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: document embedding\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "print(\"Generating document embeddings...\")\n",
    "embeddings = []\n",
    "\n",
    "for tokens in tqdm(tokenized_texts):\n",
    "    embedding = get_document_embedding(tokens, w2v_model)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each document represented as {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee374480",
   "metadata": {},
   "source": [
    "## 6. Save Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ae0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved: ../../models/word2vec/word2vec_imdb.model\n",
      "Document embeddings saved: ../../models/word2vec/document_embeddings.npy\n",
      "Labels saved: ../../models/word2vec/labels.npy\n",
      "Metadata saved: ../../models/word2vec/metadata.pkl\n",
      "\n",
      "================================================================================\n",
      "All files saved successfully!\n",
      "================================================================================\n",
      "\n",
      "Output directory: ../../models/word2vec\n",
      "Files:\n",
      "  - word2vec_imdb.model      (Word2Vec model)\n",
      "  - document_embeddings.npy  (Document embeddings: (50000, 100))\n",
      "  - labels.npy               (Sentiment labels)\n",
      "  - metadata.pkl             (Model metadata)\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = '../../models/word2vec'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save Word2Vec model\n",
    "model_path = os.path.join(output_dir, 'word2vec_imdb.model')\n",
    "w2v_model.save(model_path)\n",
    "print(f\"Word2Vec model saved: {model_path}\")\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = os.path.join(output_dir, 'document_embeddings.npy')\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"Document embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# Save labels\n",
    "labels_path = os.path.join(output_dir, 'labels.npy')\n",
    "np.save(labels_path, df['sentiment_label'].values)\n",
    "print(f\"Labels saved: {labels_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'vector_size': VECTOR_SIZE,\n",
    "    'window': WINDOW,\n",
    "    'min_count': MIN_COUNT,\n",
    "    'epochs': EPOCHS,\n",
    "    'vocab_size': len(w2v_model.wv),\n",
    "    'num_documents': len(embeddings),\n",
    "    'embedding_shape': embeddings.shape\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, 'metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - word2vec_imdb.model      (Word2Vec model)\")\n",
    "print(f\"  - document_embeddings.npy  (Document embeddings: {embeddings.shape})\")\n",
    "print(f\"  - labels.npy               (Sentiment labels)\")\n",
    "print(f\"  - metadata.pkl             (Model metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0943b",
   "metadata": {},
   "source": [
    "## 7. Test Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d80727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word similarities...\n",
      "\n",
      "Similar words to 'good':\n",
      "  decent          0.7669\n",
      "  great           0.7581\n",
      "  good,           0.7362\n",
      "  bad             0.7054\n",
      "  nice            0.6695\n",
      "\n",
      "Similar words to 'bad':\n",
      "  bad,            0.7703\n",
      "  bad.            0.7347\n",
      "  terrible        0.7218\n",
      "  horrible        0.7061\n",
      "  good            0.7054\n",
      "\n",
      "Similar words to 'movie':\n",
      "  film            0.9332\n",
      "  movie,          0.8348\n",
      "  flick           0.7746\n",
      "  film,           0.7715\n",
      "  movie.          0.7415\n",
      "\n",
      "Similar words to 'film':\n",
      "  movie           0.9332\n",
      "  film,           0.8228\n",
      "  movie,          0.7716\n",
      "  flick           0.7451\n",
      "  film;           0.7133\n",
      "\n",
      "Similar words to 'great':\n",
      "  wonderful       0.8205\n",
      "  fantastic       0.7910\n",
      "  fine            0.7849\n",
      "  terrific        0.7624\n",
      "  good            0.7581\n",
      "\n",
      "Similar words to 'terrible':\n",
      "  horrible        0.9129\n",
      "  horrid          0.8171\n",
      "  lousy           0.7659\n",
      "  awful           0.7606\n",
      "  awful,          0.7527\n",
      "\n",
      "  decent          0.7669\n",
      "  great           0.7581\n",
      "  good,           0.7362\n",
      "  bad             0.7054\n",
      "  nice            0.6695\n",
      "\n",
      "Similar words to 'bad':\n",
      "  bad,            0.7703\n",
      "  bad.            0.7347\n",
      "  terrible        0.7218\n",
      "  horrible        0.7061\n",
      "  good            0.7054\n",
      "\n",
      "Similar words to 'movie':\n",
      "  film            0.9332\n",
      "  movie,          0.8348\n",
      "  flick           0.7746\n",
      "  film,           0.7715\n",
      "  movie.          0.7415\n",
      "\n",
      "Similar words to 'film':\n",
      "  movie           0.9332\n",
      "  film,           0.8228\n",
      "  movie,          0.7716\n",
      "  flick           0.7451\n",
      "  film;           0.7133\n",
      "\n",
      "Similar words to 'great':\n",
      "  wonderful       0.8205\n",
      "  fantastic       0.7910\n",
      "  fine            0.7849\n",
      "  terrific        0.7624\n",
      "  good            0.7581\n",
      "\n",
      "Similar words to 'terrible':\n",
      "  horrible        0.9129\n",
      "  horrid          0.8171\n",
      "  lousy           0.7659\n",
      "  awful           0.7606\n",
      "  awful,          0.7527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test word similarities\n",
    "print(\"Testing word similarities...\\n\")\n",
    "\n",
    "test_words = ['good', 'bad', 'movie', 'film', 'great', 'terrible']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        print(f\"Similar words to '{word}':\")\n",
    "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"  {similar_word:15s} {score:.4f}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"'{word}' not in vocabulary\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d843c1",
   "metadata": {},
   "source": [
    "## 8. Load Saved Embeddings (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f8d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - Loading saved embeddings:\n",
      "\n",
      "✓ Word2Vec model loaded\n",
      "  Vocabulary size: 149452\n",
      "\n",
      "✓ Document embeddings loaded\n",
      "  Shape: (50000, 100)\n",
      "\n",
      "✓ Labels loaded\n",
      "  Shape: (50000,)\n",
      "\n",
      "✓ Metadata loaded\n",
      "  {'vector_size': 100, 'window': 5, 'min_count': 2, 'epochs': 10, 'vocab_size': 149452, 'num_documents': 50000, 'embedding_shape': (50000, 100)}\n",
      "\n",
      "================================================================================\n",
      "Word2Vec embedding generation complete!\n",
      "Use these embeddings for any classifier (Logistic Regression, SVM, etc.)\n",
      "================================================================================\n",
      "✓ Word2Vec model loaded\n",
      "  Vocabulary size: 149452\n",
      "\n",
      "✓ Document embeddings loaded\n",
      "  Shape: (50000, 100)\n",
      "\n",
      "✓ Labels loaded\n",
      "  Shape: (50000,)\n",
      "\n",
      "✓ Metadata loaded\n",
      "  {'vector_size': 100, 'window': 5, 'min_count': 2, 'epochs': 10, 'vocab_size': 149452, 'num_documents': 50000, 'embedding_shape': (50000, 100)}\n",
      "\n",
      "================================================================================\n",
      "Word2Vec embedding generation complete!\n",
      "Use these embeddings for any classifier (Logistic Regression, SVM, etc.)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: How to load saved embeddings\n",
    "print(\"Example - Loading saved embeddings:\\n\")\n",
    "\n",
    "# Load Word2Vec model\n",
    "loaded_model = Word2Vec.load(model_path)\n",
    "print(f\"✓ Word2Vec model loaded\")\n",
    "print(f\"  Vocabulary size: {len(loaded_model.wv)}\")\n",
    "\n",
    "# Load embeddings\n",
    "loaded_embeddings = np.load(embeddings_path)\n",
    "print(f\"\\n✓ Document embeddings loaded\")\n",
    "print(f\"  Shape: {loaded_embeddings.shape}\")\n",
    "\n",
    "# Load labels\n",
    "loaded_labels = np.load(labels_path)\n",
    "print(f\"\\n✓ Labels loaded\")\n",
    "print(f\"  Shape: {loaded_labels.shape}\")\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    loaded_metadata = pickle.load(f)\n",
    "print(f\"\\n✓ Metadata loaded\")\n",
    "print(f\"  {loaded_metadata}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Word2Vec embedding generation complete!\")\n",
    "print(\"Use these embeddings for any classifier (Logistic Regression, SVM, etc.)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
