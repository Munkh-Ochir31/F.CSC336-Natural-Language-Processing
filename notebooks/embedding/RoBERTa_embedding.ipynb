{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ef9501",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd310f7",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../../data/cleaned_label.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1854a3",
   "metadata": {},
   "source": [
    "## 3. Initialize RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c43f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name - RoBERTa Base\n",
    "MODEL_NAME = 'roberta-base'\n",
    "\n",
    "print(f\"Loading RoBERTa model: {MODEL_NAME}\")\n",
    "print(\"Note: RoBERTa is an optimized version of BERT\")\n",
    "print(\"      - No NSP (Next Sentence Prediction) task\")\n",
    "print(\"      - Dynamic masking\")\n",
    "print(\"      - Larger batch sizes & byte-level BPE\\n\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86967d2d",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1934c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embedding(text, tokenizer, model, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract RoBERTa <s> token embedding (equivalent to BERT's [CLS])\n",
    "    \n",
    "    Args:\n",
    "        text: str - Input text\n",
    "        tokenizer: RoBERTa tokenizer\n",
    "        model: RoBERTa model\n",
    "        device: torch device\n",
    "        max_length: int - Max sequence length\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: 768-dimensional embedding vector\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract <s> token embedding (first token, like BERT's [CLS])\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Embedding extraction function defined!\")\n",
    "\n",
    "# Test on sample text\n",
    "sample_text = \"This movie is absolutely fantastic!\"\n",
    "sample_embedding = get_roberta_embedding(sample_text, tokenizer, model, device)\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Sample embedding preview (first 10 values): {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef42d4",
   "metadata": {},
   "source": [
    "## 5. Extract Embeddings for All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32  # Process in batches to save memory\n",
    "\n",
    "print(\"Extracting RoBERTa embeddings for all documents...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total documents: {len(df)}\\n\")\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    batch_texts = df['review_text'].iloc[i:i+BATCH_SIZE].values\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        embedding = get_roberta_embedding(str(text), tokenizer, model, device, MAX_LENGTH)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"\\nEmbeddings extracted!\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Each document: {embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337b6e2",
   "metadata": {},
   "source": [
    "## 6. Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = '../../models/roberta_embeddings'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = os.path.join(output_dir, 'roberta_embeddings.npy')\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# Save labels\n",
    "labels_path = os.path.join(output_dir, 'labels.npy')\n",
    "np.save(labels_path, df['sentiment_label'].values)\n",
    "print(f\"Labels saved: {labels_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_type': 'roberta-base',\n",
    "    'hidden_size': model.config.hidden_size,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'num_documents': len(embeddings),\n",
    "    'embedding_shape': embeddings.shape,\n",
    "    'extraction_method': '<s> token from last_hidden_state',\n",
    "    'notes': 'RoBERTa optimizations: no NSP, dynamic masking, BPE'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, 'metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - roberta_embeddings.npy    (Embeddings: {embeddings.shape})\")\n",
    "print(f\"  - labels.npy                (Sentiment labels)\")\n",
    "print(f\"  - metadata.pkl              (Model metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eecf8",
   "metadata": {},
   "source": [
    "## 7. Verify Saved Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b98e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings\n",
    "print(\"Loading saved embeddings...\\n\")\n",
    "\n",
    "loaded_embeddings = np.load(embeddings_path)\n",
    "loaded_labels = np.load(labels_path)\n",
    "\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    loaded_metadata = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Embeddings loaded: {loaded_embeddings.shape}\")\n",
    "print(f\"‚úì Labels loaded: {loaded_labels.shape}\")\n",
    "print(f\"\\n‚úì Metadata:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Verify integrity\n",
    "print(f\"\\n‚úì Verification:\")\n",
    "print(f\"    Embeddings match: {np.allclose(embeddings, loaded_embeddings)}\")\n",
    "print(f\"    Labels match: {np.array_equal(df['sentiment_label'].values, loaded_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04c3e8",
   "metadata": {},
   "source": [
    "## 8. Embedding Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Embedding Statistics:\\n\")\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"Std: {embeddings.std():.4f}\")\n",
    "print(f\"Min: {embeddings.min():.4f}\")\n",
    "print(f\"Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(embeddings.flatten(), bins=100, alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Embedding Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('RoBERTa Embedding Value Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean embedding per document\n",
    "mean_embeddings = embeddings.mean(axis=1)\n",
    "axes[1].hist(mean_embeddings, bins=50, alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Mean Embedding Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Mean Embedding per Document')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistics plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9b663",
   "metadata": {},
   "source": [
    "## 9. Compare Positive vs Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample for visualization\n",
    "sample_size = 1000\n",
    "indices = np.random.choice(len(embeddings), size=min(sample_size, len(embeddings)), replace=False)\n",
    "\n",
    "sample_embeddings = embeddings[indices]\n",
    "sample_labels = df['sentiment_label'].iloc[indices].values\n",
    "\n",
    "# PCA reduction to 2D\n",
    "print(f\"Performing PCA on {len(sample_embeddings)} samples...\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(sample_embeddings)\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=sample_labels,\n",
    "    cmap='RdYlGn',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "plt.colorbar(scatter, label='Sentiment (0=Neg, 1=Pos)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('RoBERTa Embeddings Visualization (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f022b7",
   "metadata": {},
   "source": [
    "## 10. RoBERTa vs BERT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RoBERTa vs BERT Differences\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = {\n",
    "    'Feature': [\n",
    "        'Training Data',\n",
    "        'Training Time',\n",
    "        'NSP Task',\n",
    "        'Masking',\n",
    "        'Tokenization',\n",
    "        'Batch Size',\n",
    "        'Performance'\n",
    "    ],\n",
    "    'BERT': [\n",
    "        'BookCorpus + Wikipedia (16GB)',\n",
    "        '1M steps',\n",
    "        'Yes (Next Sentence Prediction)',\n",
    "        'Static (same for each epoch)',\n",
    "        'WordPiece',\n",
    "        'Smaller',\n",
    "        'Good'\n",
    "    ],\n",
    "    'RoBERTa': [\n",
    "        'CC-News + Other (160GB)',\n",
    "        '500K steps',\n",
    "        'No (removed)',\n",
    "        'Dynamic (changes each epoch)',\n",
    "        'Byte-level BPE',\n",
    "        'Larger (8K)',\n",
    "        'Better'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Advantages of RoBERTa:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. 10x more training data\")\n",
    "print(\"2. Removed NSP task (not helpful)\")\n",
    "print(\"3. Dynamic masking (better generalization)\")\n",
    "print(\"4. Byte-level BPE (handles rare words better)\")\n",
    "print(\"5. Larger batch sizes (more stable training)\")\n",
    "print(\"\\nResult: RoBERTa typically outperforms BERT on downstream tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62476d",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30203126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RoBERTa EMBEDDING EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Model: {MODEL_NAME}\")\n",
    "print(f\"üöÄ Architecture: Robustly Optimized BERT\")\n",
    "print(f\"üìè Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"üìÅ Total documents: {embeddings.shape[0]:,}\")\n",
    "print(f\"üíæ Total size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Files saved:\")\n",
    "print(f\"   {output_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ roberta_embeddings.npy  ({embeddings.shape})\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ labels.npy              ({loaded_labels.shape})\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ metadata.pkl\")\n",
    "\n",
    "print(f\"\\nüí° Usage:\")\n",
    "print(f\"   These embeddings can be used with any classifier:\")\n",
    "print(f\"   - Logistic Regression\")\n",
    "print(f\"   - SVM\")\n",
    "print(f\"   - Random Forest\")\n",
    "print(f\"   - Neural Networks\")\n",
    "\n",
    "print(f\"\\nüîß Load embeddings:\")\n",
    "print(f\"   X = np.load('{embeddings_path}')\")\n",
    "print(f\"   y = np.load('{labels_path}')\")\n",
    "\n",
    "print(f\"\\n‚ö° Why RoBERTa:\")\n",
    "print(f\"   - More robust than BERT\")\n",
    "print(f\"   - Better handling of rare words\")\n",
    "print(f\"   - Improved performance on most tasks\")\n",
    "print(f\"   - State-of-the-art when it was released (2019)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ RoBERTa embedding extraction complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
