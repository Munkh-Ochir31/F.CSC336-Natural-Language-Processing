# BERT Models Sentiment Analysis - Usage Guide

–≠–Ω—ç guide –Ω—å `src/models/` –¥–æ—Ç–æ—Ä –±–∞–π–≥–∞–∞ BERT –º–æ–¥—É–ª–∏—É–¥—ã–≥ —Ö—ç—Ä—Ö—ç–Ω –∞—à–∏–≥–ª–∞—Ö —Ç–∞–ª–∞–∞—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞—Å–∞–Ω.

## üìÅ –§–∞–π–ª—ã–Ω –±“Ø—Ç—ç—Ü

```
src/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ bert_models.py    # BERT –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω –∫–ª–∞—Å—Å—É—É–¥
‚îÇ   ‚îú‚îÄ‚îÄ predict.py        # Prediction —Å–∫—Ä–∏–ø—Ç
‚îÇ   ‚îî‚îÄ‚îÄ train.py          # Training —Å–∫—Ä–∏–ø—Ç
```

## üöÄ –•—ç—Ä—ç–≥–ª—ç—ç–Ω–∏–π –∂–∏—à—ç—ç

### 1. Python –∫–æ–¥ –¥–æ—Ç–æ—Ä –∞—à–∏–≥–ª–∞—Ö

#### –≠–Ω–≥–∏–π–Ω prediction:

```python
from src.models.bert_models import BERTSentimentModel

# BERT –∑–∞–≥–≤–∞—Ä –∞—á–∞–∞–ª–∞—Ö
model = BERTSentimentModel('bert-base-uncased')

# –¢–µ–∫—Å—Ç —Ç–∞–∞—Ö
text = "This movie is fantastic!"
result = model.predict(text)

print(f"Sentiment: {result['sentiment']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Probabilities: {result['probabilities']}")
```

**–ì–∞—Ä–∞–ª—Ç:**
```
Sentiment: positive
Confidence: 98.50%
Probabilities: {'negative': 0.015, 'positive': 0.985}
```

#### –û–ª–æ–Ω —Ç–µ–∫—Å—Ç prediction:

```python
texts = [
    "Great movie!",
    "Terrible waste of time",
    "It was okay"
]

results = model.predict_batch(texts)

for text, result in zip(texts, results):
    print(f"{text} -> {result['sentiment']} ({result['confidence']:.2%})")
```

#### –ë“Ø—Ö BERT –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö:

```python
from src.models.bert_models import BERTSentimentModel, compare_models

# –ó–∞–≥–≤–∞—Ä—É—É–¥ “Ø“Ø—Å–≥—ç—Ö
models = {
    'BERT': BERTSentimentModel('bert-base-uncased'),
    'BERT Cased': BERTSentimentModel('bert-base-cased'),
    'RoBERTa': BERTSentimentModel('roberta-base'),
    'ALBERT': BERTSentimentModel('albert-base-v2'),
    'HateBERT': BERTSentimentModel('GroNLP/hateBERT')
}

# –•–∞—Ä—å—Ü—É—É–ª–∞—Ö
text = "This movie is amazing!"
results = compare_models(text, models)

for name, result in results.items():
    print(f"{name}: {result['sentiment']} ({result['confidence']:.2%})")
```

#### SBERT –∞—à–∏–≥–ª–∞—Ö:

```python
from src.models.bert_models import SBERTSentimentModel

# SBERT –∑–∞–≥–≤–∞—Ä + classifier
model = SBERTSentimentModel('all-MiniLM-L6-v2')
model.load_classifier('./notebooks/sbert_classifier.pkl')

result = model.predict("This is a great film!")
print(result)
```

### 2. Command line –∞—à–∏–≥–ª–∞—Ö

#### –ù—ç–≥ —Ç–µ–∫—Å—Ç prediction:

```bash
# BERT-—ç—ç—Ä prediction
python src/models/predict.py --text "This movie is amazing!" --model bert-base-uncased

# RoBERTa-–∞–∞—Ä prediction
python src/models/predict.py --text "Terrible movie" --model roberta-base

# HateBERT-—ç—ç—Ä prediction
python src/models/predict.py --text "This is garbage" --model GroNLP/hateBERT
```

#### –ë“Ø—Ö –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö:

```bash
python src/models/predict.py --text "This movie is fantastic!" --compare
```

**–ì–∞—Ä–∞–ª—Ç:**
```
================================================================================
Comparing all BERT models...
================================================================================
Text: This movie is fantastic!

Loading BERT Base Uncased...
  ‚úì positive (confidence: 99.12%)
Loading BERT Base Cased...
  ‚úì positive (confidence: 98.87%)
Loading RoBERTa...
  ‚úì positive (confidence: 99.45%)
Loading ALBERT...
  ‚úì positive (confidence: 97.23%)
Loading HateBERT...
  ‚úì positive (confidence: 98.91%)

================================================================================
Summary:
================================================================================
üü¢ BERT Base Uncased   : positive   (99.12%)
üü¢ BERT Base Cased     : positive   (98.87%)
üü¢ RoBERTa             : positive   (99.45%)
üü¢ ALBERT              : positive   (97.23%)
üü¢ HateBERT            : positive   (98.91%)
```

#### –û–ª–æ–Ω —Ç–µ–∫—Å—Ç file-–∞–∞—Å —É–Ω—à–∏—Ö:

```bash
# texts.txt —Ñ–∞–π–ª “Ø“Ø—Å–≥—ç—Ö
echo "This movie is great!" > texts.txt
echo "Terrible waste of time" >> texts.txt
echo "It was okay" >> texts.txt

# Prediction —Ö–∏–π—Ö
python src/models/predict.py --file texts.txt --model bert-base-uncased --output results.txt
```

#### Interactive –≥–æ—Ä–∏–º:

```bash
python src/models/predict.py --interactive
```

**–ñ–∏—à—ç—ç:**
```
================================================================================
BERT Sentiment Analysis - Interactive Mode
================================================================================

Available models:
  1. BERT Base Uncased
  2. BERT Base Cased
  3. RoBERTa
  4. ALBERT
  5. HateBERT
  6. Compare All

Type 'quit' to exit

Select model (1-6): 1
Enter text: This movie is amazing!

üü¢ Prediction: POSITIVE
   Confidence: 99.12%
   Probabilities:
     - Negative: 0.88%
     - Positive: 99.12%
```

### 3. –°—É—Ä–≥–∞—Å–∞–Ω –º–æ–¥–µ–ª –∞—à–∏–≥–ª–∞—Ö

Colab –¥—ç—ç—Ä —Å—É—Ä–≥–∞–∞–¥ —Ç–∞—Ç–∞–∂ –∞–≤—Å–∞–Ω –º–æ–¥–µ–ª:

```python
from src.models.bert_models import BERTSentimentModel

# –°—É—Ä–≥–∞—Å–∞–Ω –º–æ–¥–µ–ª –∞—á–∞–∞–ª–∞—Ö
model = BERTSentimentModel('./notebooks/bert_sentiment')

# Prediction —Ö–∏–π—Ö
result = model.predict("This is a great movie!")
print(result)
```

Command line:

```bash
python src/models/predict.py \
  --text "Great movie!" \
  --model-path ./notebooks/bert_sentiment
```

### 4. –®–∏–Ω—ç –º–æ–¥–µ–ª —Å—É—Ä–≥–∞—Ö

```bash
# BERT —Å—É—Ä–≥–∞—Ö
python src/models/train.py \
  --data data/cleaned_label.csv \
  --model bert-base-uncased \
  --output ./models/my_bert \
  --epochs 3 \
  --batch-size 16

# RoBERTa —Å—É—Ä–≥–∞—Ö
python src/models/train.py \
  --data data/cleaned_label.csv \
  --model roberta-base \
  --output ./models/my_roberta \
  --epochs 3

# ALBERT —Å—É—Ä–≥–∞—Ö
python src/models/train.py \
  --data data/cleaned_label.csv \
  --model albert-base-v2 \
  --output ./models/my_albert \
  --epochs 3
```

## üìä –ë“Ø—Ö 6 –∑–∞–≥–≤–∞—Ä—ã–Ω —Ö–∞–º—Ä–∞—Ö —Ö“Ø—Ä—ç—ç

| –ó–∞–≥–≤–∞—Ä | Model Name | Parameters | –û–Ω—Ü–ª–æ–≥ |
|--------|-----------|-----------|--------|
| BERT Base Uncased | `bert-base-uncased` | 110M | –°—Ç–∞–Ω–¥–∞—Ä—Ç BERT, –∂–∏–∂–∏–≥ “Ø—Å–≥—ç—ç—Ä |
| BERT Base Cased | `bert-base-cased` | 110M | –¢–æ–º –∂–∏–∂–∏–≥ “Ø—Å—ç–≥ —è–ª–≥–∞–¥–∞–≥ |
| RoBERTa | `roberta-base` | 125M | –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω BERT |
| SBERT | `all-MiniLM-L6-v2` | 22M | Sentence embeddings |
| ALBERT | `albert-base-v2` | 12M | –ñ–∏–∂–∏–≥, —Ö—É—Ä–¥–∞–Ω |
| HateBERT | `GroNLP/hateBERT` | 110M | Toxic content-–¥ —Å–∞–π–Ω |

## üîß –î–∞–≤—É—É —Ç–∞–ª—É—É–¥

### BERT Base Uncased
- ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç, ”©—Ä–≥”©–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥–¥–¥—ç–≥
- ‚úÖ –°–∞–π–Ω –µ—Ä”©–Ω—Ö–∏–π “Ø—Ä –¥“Ø–Ω

### BERT Base Cased
- ‚úÖ –¢–æ–º –∂–∏–∂–∏–≥ “Ø—Å—ç–≥ —è–ª–≥–∞–¥–∞–≥
- ‚úÖ –ù—ç—Ä —Ç–æ–º—ä—ë–æ —Å–∞–π–Ω —Ç–∞–Ω—å–¥–∞–≥

### RoBERTa
- ‚úÖ BERT-—ç—ç—Å —Å–∞–π–Ω
- ‚úÖ –ò–ª“Ø“Ø —Å–∞–π–Ω pretraining

### SBERT
- ‚úÖ –ú–∞—à —Ö—É—Ä–¥–∞–Ω (500x faster than BERT)
- ‚úÖ Sentence similarity
- ‚úÖ Semantic search

### ALBERT
- ‚úÖ –•–∞–º–≥–∏–π–Ω –∂–∏–∂–∏–≥ (12M parameters)
- ‚úÖ –•–∞–º–≥–∏–π–Ω —Ö—É—Ä–¥–∞–Ω
- ‚úÖ Parameter sharing

### HateBERT
- ‚úÖ –°”©—Ä”©–≥ —Å—ç—Ç–≥—ç–≥–¥—ç–ª –∏–ª“Ø“Ø —Å–∞–π–Ω
- ‚úÖ Toxic/offensive language
- ‚úÖ Hate speech detection

## üí° –ó”©–≤–ª”©–º–∂

1. **–≠–Ω–≥–∏–π–Ω sentiment analysis**: BERT Base Uncased
2. **–•—É—Ä–¥–∞–Ω inference**: ALBERT —ç—Å–≤—ç–ª SBERT
3. **–°”©—Ä”©–≥ —Å—ç—Ç–≥—ç–≥–¥—ç–ª –∏–ª“Ø“Ø —Å–∞–π–Ω**: HateBERT
4. **–ï—Ä”©–Ω—Ö–∏–π —Å–∞–π–Ω “Ø—Ä –¥“Ø–Ω**: RoBERTa
5. **Semantic search/similarity**: SBERT

## üéØ ”®—Ä–≥”©–Ω —Ö—ç—Ä—ç–≥–ª—ç—ç–Ω–∏–π –∂–∏—à—ç—ç

### Web API “Ø“Ø—Å–≥—ç—Ö:

```python
from flask import Flask, request, jsonify
from src.models.bert_models import BERTSentimentModel

app = Flask(__name__)
model = BERTSentimentModel('roberta-base')

@app.route('/predict', methods=['POST'])
def predict():
    text = request.json.get('text', '')
    result = model.predict(text)
    return jsonify(result)

if __name__ == '__main__':
    app.run(debug=True)
```

### Batch processing:

```python
import pandas as pd
from src.models.bert_models import BERTSentimentModel

# CSV —É–Ω—à–∏—Ö
df = pd.read_csv('reviews.csv')

# –ú–æ–¥–µ–ª –∞—á–∞–∞–ª–∞—Ö
model = BERTSentimentModel('roberta-base')

# Batch prediction
results = model.predict_batch(df['review_text'].tolist(), batch_size=32)

# “Æ—Ä –¥“Ø–Ω –Ω—ç–º—ç—Ö
df['sentiment'] = [r['sentiment'] for r in results]
df['confidence'] = [r['confidence'] for r in results]

# –•–∞–¥–≥–∞–ª–∞—Ö
df.to_csv('reviews_with_sentiment.csv', index=False)
```

## üêõ Troubleshooting

**CUDA out of memory:**
```python
# Batch size-–≥ –±–∞–≥–∞—Å–≥–∞—Ö
model.predict_batch(texts, batch_size=8)
```

**–ú–æ–¥–µ–ª –æ–ª–¥–æ—Ö–≥“Ø–π:**
```python
# –≠—Ö–ª—ç—ç–¥ —Ç–∞—Ç–∞–∂ –∞–≤–∞—Ö
from transformers import AutoModel
AutoModel.from_pretrained('bert-base-uncased')
```

**Import –∞–ª–¥–∞–∞:**
```python
import sys
sys.path.append('/path/to/biydaalt1')
from src.models.bert_models import BERTSentimentModel
```

–ê–º–∂–∏–ª—Ç —Ö“Ø—Å—å–µ! üöÄ
